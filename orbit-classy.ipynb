{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "In this notebook, we will explore the process of generating and classifying orbits using **Persistence Signals** vectorization and a Vanilla Random Forest Classifier. The main steps include:\n",
    "\n",
    "1. *Data Generation*: Creating synthetic orbit data.\n",
    "2. *Data Processing*: Preparing the data by computing persistent diagrams for $H_0$ and $H_1$ for each sample.\n",
    "3. *Diagrams Embedding*: Each diagram from the dataset is embedded in a vector space using Persistence Signals.\n",
    "3. *Model Training and Evaluation*: Building and training a RandomForest classifier and assess the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spectral.utils as spu\n",
    "from gudhi.representations.preprocessing import BirthPersistenceTransform\n",
    "from spectral.signals import Signals\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "dataset = \"ORBIT5K\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "In this section, we generate synthetic data representing different orbits. This involves creating a series of points that simulate the path of an object in an orbital motion, based on specified parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1000 dynamical particles and PDs for r = 2.5...\n",
      "Generating 1000 dynamical particles and PDs for r = 3.5...\n",
      "Generating 1000 dynamical particles and PDs for r = 4.0...\n",
      "Generating 1000 dynamical particles and PDs for r = 4.1...\n",
      "Generating 1000 dynamical particles and PDs for r = 4.3...\n"
     ]
    }
   ],
   "source": [
    "spu.compute_persistence(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "In the plot below, a representative sample from each class is displayed to illustrate the diversity and characteristics of the different categories in our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Optional Image Title](data/ORBIT5K/orbits.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "Here, we prepare the generated data for analysis by computing persistent diagrams for $H_0$ and $H_1$ homology groups for each sample. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ORBIT5K\n",
      "Number of observations: 5000\n",
      "Number of classes: 5\n"
     ]
    }
   ],
   "source": [
    "diagrams_dict, labels, n_data = spu.get_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagrams Embedding\n",
    "In this step, each persistence diagram from the dataset is embedded in a vector space using the **Persistence Signals** vectorization method. This transformation facilitates the application of machine learning algorithms by representing the complex topological data in a more accessible form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "max_point = spu.max_measures(diagrams_dict)\n",
    "shift = BirthPersistenceTransform()\n",
    "model = Signals(function = \"WAVELET\",\n",
    "                resolution = (128, 128),\n",
    "                global_max = max_point,\n",
    "                wave = \"coif2\")\n",
    "for i in range(n_data):\n",
    "    PD0, PD1 = diagrams_dict[\"H0\"][i], diagrams_dict[\"H1\"][i]\n",
    "    PD0 = PD0[~np.isinf(PD0).any(axis=1)]\n",
    "    shift.fit([PD0, PD1])\n",
    "    diagrms = shift.transform([PD0, PD1])\n",
    "    model.fit(diagrms)\n",
    "    data.append(model.transform(diagrms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation\n",
    "In this section, we focus on both building and evaluating our classifier. Initially, we train the classifier using the processed data. Subsequently, we assess its performance by evaluating its accuracy and reliability in classifying different types of orbits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: Accuracy = 0.848\n",
      "Run 2: Accuracy = 0.8453333333333334\n",
      "Run 3: Accuracy = 0.83\n",
      "Run 4: Accuracy = 0.8373333333333334\n",
      "Run 5: Accuracy = 0.8413333333333334\n",
      "Run 6: Accuracy = 0.8293333333333334\n",
      "Run 7: Accuracy = 0.8313333333333334\n",
      "Run 8: Accuracy = 0.8326666666666667\n",
      "Run 9: Accuracy = 0.8413333333333334\n",
      "Run 10: Accuracy = 0.8446666666666667\n",
      "Overall Mean Accuracy across 10 runs: 0.8381333333333334\n",
      "Standard Deviation across 10 runs: 0.006578078071223475\n"
     ]
    }
   ],
   "source": [
    "mean, std = spu.evaluate_classifier_orbits(data, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
